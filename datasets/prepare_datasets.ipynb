{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_file_datasets(file_path, sep=\" \"):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        file = f.readlines()\n",
    "    data = list(map(lambda s : s.split(sep), file))\n",
    "    data = list(map(lambda s : [float(s_i.strip()) if is_number(s_i) else s_i.strip() for s_i in s], data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenWebText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"openwebtext\", cache_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATICWORDEMBEDDINGSEVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ws353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./ws353simrel/wordsim353_annotator2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _read_file_datasets(file_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'love', 'sex', 6.77]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./MEN/MEN_dataset_natural_form_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _read_file_datasets(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sun', 'sunlight', 50.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./SIMLEX/SimLex-999.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _read_file_datasets(file_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['old', 'new', 'A', 1.58, 2.72, 2.81, 2.0, 7.25, 1.0, 0.41]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./RW/rw.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _read_file_datasets(file_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squishing', 'squirt', 5.88, 7.0, 7.0, 6.0, 1.0, 4.0, 6.0, 6.0, 7.0, 2.0, 4.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./RG/rg65.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _read_file_datasets(file_path, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cord', 'smile', 0.02]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mturk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Mturk/EN-MTurk-287.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _read_file_datasets(file_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['episcopal', 'russia', 2.75]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXT-DEPENDENT WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WiC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./WiC/train/train.data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = _read_file_datasets(train_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carry',\n",
       " 'V',\n",
       " '2-1',\n",
       " 'You must carry your camping gear .',\n",
       " 'Sound carries well over water .']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"SCWS/ratings.txt\"\n",
    "train_data = _read_file_datasets(train_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 'Brazil',\n",
       " 'n',\n",
       " 'nut',\n",
       " 'n',\n",
       " 'gap in income between blacks and other non-whites is relatively small compared to the large gap between whites and non-whites . Other factors such as illiteracy and education level show the same patterns . Unlike in the US where African Americans were united in the civil rights struggle , in <b> Brazil </b> the philosophy of whitening has helped divide blacks from other non-whites and prevented a more active civil rights movement . Though Afro-Brazilians make up half the population there are very few black politicians . The city of Salvador , Bahia for instance is 80 % Afro-Brazilian but has never',\n",
       " 'of the neck , bridge , and pickups , there are features which are found in almost every guitar . The photo below shows the different parts of an electric guitar . The headstock ( 1 ) contains the metal machine heads , which are used for tuning ; the <b> nut </b> ( 1.4 ) , a thin fret-like strip of metal , plastic , graphite or bone which the strings pass over as they first go onto the fingerboard ; the machine heads ( 1.1 ) , which are worm gears which the player turns to change the string tension',\n",
       " 1.1,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 6.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newsgroupDATASET(Dataset):\n",
    "    def __init__(self, news, labels):\n",
    "        self.news = news\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.news[index], self.labels[index]\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroup_train_X, newsgroup_train_y = fetch_20newsgroups(subset=\"train\", download_if_missing=False, \n",
    "                                                          return_X_y=True, shuffle=True,\n",
    "                                                          random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroup_test_X, newsgroup_test_y = fetch_20newsgroups(subset=\"test\", download_if_missing=False, \n",
    "                                                        return_X_y=True, shuffle=True,\n",
    "                                                        random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of news, tensor of labels\n",
    "train_loader = DataLoader(newsgroupDATASET(newsgroup_train_X, newsgroup_train_y), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(newsgroupDATASET(newsgroup_test_X, newsgroup_test_y), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length news batch: 32\n",
      "Targets: tensor([17,  0, 17, 11, 10, 15,  4, 17, 13, 12,  1,  6, 13, 15,  4, 11, 11, 10,\n",
      "        12, 19, 12, 12, 16, 16, 10, 15,  6, 11, 10,  8, 11,  4])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(\"Length news batch:\", len(x))\n",
    "    print(\"Targets:\", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOS-11967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WOSDATASET(Dataset):\n",
    "    def __init__(self, documents, labels):\n",
    "        self.documents = documents\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.documents[index], self.labels[index]\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_examples(input_file, label_file, label_level_1_file, label_level_2_file):\n",
    "        \"\"\"Yields examples.\"\"\"\n",
    "        with open(input_file, encoding=\"utf-8\") as f:\n",
    "            input_data = f.readlines()\n",
    "        with open(label_file, encoding=\"utf-8\") as f:\n",
    "            label_data = f.readlines()\n",
    "        with open(label_level_1_file, encoding=\"utf-8\") as f:\n",
    "            label_level_1_data = f.readlines()\n",
    "        with open(label_level_2_file, encoding=\"utf-8\") as f:\n",
    "            label_level_2_data = f.readlines()\n",
    "        for i in range(len(input_data)):\n",
    "            yield i, {\n",
    "                \"input_data\": input_data[i],\n",
    "                \"label\": label_data[i],\n",
    "                \"label_level_1\": label_level_1_data[i],\n",
    "                \"label_level_2\": label_level_2_data[i],\n",
    "            }\n",
    "            \n",
    "def _read_data(input_file, label_file, label_level_1_file, label_level_2_file):\n",
    "    with open(input_file, encoding=\"utf-8\") as f:\n",
    "        input_data = f.readlines()\n",
    "    with open(label_file, encoding=\"utf-8\") as f:\n",
    "        label_data = f.readlines()\n",
    "        label_data = list(map(lambda s: int(s.strip()), label_data))\n",
    "    with open(label_level_1_file, encoding=\"utf-8\") as f:\n",
    "        label_level_1_data = f.readlines()\n",
    "    with open(label_level_2_file, encoding=\"utf-8\") as f:\n",
    "        label_level_2_data = f.readlines()\n",
    "    return input_data, label_data, label_level_1_data, label_level_2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"./WOS11967/\"\n",
    "input_file = dir_path + \"X.txt\"\n",
    "label_file = dir_path + \"Y.txt\"\n",
    "label_level_1_file = dir_path + \"YL1.txt\"\n",
    "label_level_2_file = dir_path + \"YL2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = _generate_examples(input_file, label_file, label_level_1_file, label_level_2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    print(example[1][\"label\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, label_data, label_level_1_data, label_level_2_data = _read_data(input_file, label_file, label_level_1_file, label_level_2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train, input_data_test, label_data_train, label_data_test = train_test_split(input_data, \n",
    "                                                                                        label_data,\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        stratify=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of documents, tensor of labels\n",
    "train_loader = DataLoader(WOSDATASET(input_data_train, label_data_train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(WOSDATASET(input_data_test, label_data_test), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length documents batch: 32\n",
      "Example : \n",
      " A high-altitude (>1,500 m asl) integrated participatory watershed development programme was implemented between 2004 and 2008 in the West Khasi Hills, Meghalaya, North-Eastern Indian Himalaya. The aim was to assess and refine practices for integrating crop, fish and livestock production systems. Soil and water conservation measures, with the active participation of local inhabitants, included the construction and renovation of ponds, jalkunds (micro rainwater-harvesting structures) and bench and half-moon terraces. Impact analysis revealed that 4.3 million litres of water were harvested and enhanced potato and rice crop productivity by 30% to 40% and 45% to 50% respectively. Farmers are now able to earn net incomes of around $56.8 and $8.9 per month from community dairy units and fish ponds respectively.\n",
      "\n",
      "Targets: tensor([21, 10, 12,  2,  6,  5, 24,  2, 29,  1, 28, 19, 24,  4,  9, 12, 18,  7,\n",
      "         8, 16,  4,  1, 19, 29,  0, 18, 27,  8,  9, 18, 13,  9])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(\"Length documents batch:\", len(x))\n",
    "    print(\"Example : \\n\", x[0])\n",
    "    print(\"Targets:\", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC-6 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TREC6DATASET(Dataset):\n",
    "    def __init__(self, questions, labels):\n",
    "        self.questions = questions\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.questions[index], self.labels[index]\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"TREC-6/train.txt\"\n",
    "test_file = \"TREC-6/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data_name):\n",
    "    features = []\n",
    "    lbl = []\n",
    "    with codecs.open(data_name, 'r', encoding=\"latin-1\") as f:\n",
    "        for line in f:\n",
    "            words = clean_str(line.strip())[2:]\n",
    "            y = int(line[0])\n",
    "            features.append(words)\n",
    "            lbl.append(y)\n",
    "    return features, lbl\n",
    "\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "train_input, train_output = convert_data(train_file)\n",
    "test_input, test_output = convert_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of documents, tensor of labels\n",
    "train_loader = DataLoader(WOSDATASET(train_input, train_output), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(WOSDATASET(test_input, train_output), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length questions batch: 32\n",
      "Example : \n",
      " What city did the Mormons establish as their headquarters in 1847 ?\n",
      "Targets: tensor([4, 0, 3, 1, 1, 4, 5, 0, 3, 0, 3, 3, 0, 0, 3, 0, 0, 0, 1, 4, 0, 1, 0, 3,\n",
      "        4, 0, 3, 5, 4, 5, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(\"Length questions batch:\", len(x))\n",
    "    print(\"Example : \\n\", x[0])\n",
    "    print(\"Targets:\", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTDATASET(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.reviews[index], self.labels[index]\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"SST/train.txt\"\n",
    "test_file = \"SST/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "train_input, train_output = convert_data(train_file)\n",
    "test_input, test_output = convert_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of documents, tensor of labels\n",
    "train_loader = DataLoader(SSTDATASET(train_input, train_output), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(SSTDATASET(test_input, train_output), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length reviews batch: 32\n",
      "Example : \n",
      " a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
      "Targets: tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(\"Length reviews batch:\", len(x))\n",
    "    print(\"Example : \\n\", x[0])\n",
    "    print(\"Targets:\", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
