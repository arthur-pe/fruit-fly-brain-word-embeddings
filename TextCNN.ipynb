{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "from fruitFlyVectorizer import FruitFlyVectorizer\n",
    "from model import KCnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords=set(stopwords.words(\"english\"))\n",
    "detokenizer=TreebankWordDetokenizer()\n",
    "\n",
    "def clean(x):\n",
    "    x= x.lower()\n",
    "    x= re.sub(\"[^ \\w]\",\" \",x)\n",
    "    x=re.sub(\"(\\s\\d+\\s|^\\d+\\s)\", \" \", x)\n",
    "    x=re.sub(\" \\d+\", \" <NUM> \", x) \n",
    "    x= re.sub(\"  \",\" \",x)\n",
    "    words= word_tokenize(x)\n",
    "    words = [w for w in words if not w in english_stopwords]\n",
    "    clean_x = detokenizer.detokenize(words)\n",
    "    length= len(words)\n",
    "    return clean_x,length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN de Yoon Kim : https://arxiv.org/abs/1408.5882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.conv_0 = torch.nn.Conv1d(in_channels = embedding_dim, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = filter_sizes[0])\n",
    "        \n",
    "        self.conv_1 = torch.nn.Conv1d(in_channels = embedding_dim, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = filter_sizes[1])\n",
    "        \n",
    "        self.conv_2 = torch.nn.Conv1d(in_channels = embedding_dim, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = filter_sizes[2])\n",
    "        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, embedded):\n",
    "        embedded= embedded.to(device)     \n",
    "        # batch_size, len, embdim\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # batch_size, n_filters, len - filter_sizes[n] + 1\n",
    "        conved_0 = F.relu(self.conv_0(embedded))\n",
    "        conved_1 = F.relu(self.conv_1(embedded))\n",
    "        conved_2 = F.relu(self.conv_2(embedded))\n",
    "        \n",
    "        # batch_size, n_filters   \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        # batch_size, n_filters * len(filter_sizes)\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_norm(named_parameters, lambda_norm, dim):\n",
    "    \"\"\"\n",
    "        Calcul de la norme l_dim avec un lambda_norm\n",
    "    \"\"\"\n",
    "    l_reg = torch.tensor(0., requires_grad=False, device=device)\n",
    "    for name, param in named_parameters:\n",
    "        if 'weight' in name:\n",
    "            l_reg += torch.norm(param, dim)\n",
    "            \n",
    "    return lambda_norm * l_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Fonction de train d'une epoch dans le cas d'une multiclass classification\n",
    "    \"\"\"\n",
    "    lambda_norm = 1e-3\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for x_train, y_train in tqdm(train_loader):\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        X_embedding = torch.cat([kcmodel.Hash(x_i, k).unsqueeze(0) for x_i in x_train], dim=0)\n",
    "        predictions = model(X_embedding).squeeze(1)\n",
    "        loss = criterion(predictions, y_train)\n",
    "        loss += l_norm(model.named_parameters(), lambda_norm, dim=2)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        acc = accuracy_score(y_train.cpu(), predictions.cpu())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)\n",
    "\n",
    "def train_binary(model, train_loader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Fonction de train d'une epoch dans le cas d'une classification binaire \n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for x_train, y_train in tqdm(train_loader):\n",
    "        \n",
    "        y_train = y_train.to(torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        X_embedding = torch.cat([kcmodel.Hash(x_i, k).unsqueeze(0) for x_i in x_train], dim=0)\n",
    "        predictions = model(X_embedding).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (predictions == y_train).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Fonction d'evaluation du modèle sur le testset dans le cas d'une multiclass classification\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for x_test, y_test in tqdm(test_loader):\n",
    "            \n",
    "            X_embedding = torch.cat([kcmodel.Hash(x_i, k).unsqueeze(0) for x_i in x_test], dim=0)\n",
    "            predictions = model(X_embedding).squeeze(1)\n",
    "            loss = criterion(predictions, y_test)\n",
    "            \n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            acc = accuracy_score(y_test.cpu(), predictions.cpu())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(test_loader), epoch_acc / len(test_loader)\n",
    "\n",
    "def evaluate_binary(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Fonction d'évaluation du modèle sur le testset dans le cas d'une classification binaire\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for x_test, y_test in tqdm(test_loader):\n",
    "            \n",
    "            y_test = y_test.to(torch.float32)\n",
    "            X_embedding = torch.cat([kcmodel.Hash(x_i, k).unsqueeze(0) for x_i in x_test], dim=0)\n",
    "            predictions = model(X_embedding).squeeze(1)\n",
    "            loss = criterion(predictions, y_test)\n",
    "            \n",
    "            predictions = torch.round(torch.sigmoid(predictions))\n",
    "            correct = (predictions == y_test).float()\n",
    "            acc = correct.sum() / len(correct)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.cpu()\n",
    "        \n",
    "    return epoch_loss / len(test_loader), epoch_acc / len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload du FruitFlyVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= joblib.load(\"./fruitFlyVectorizer_window=10.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_hidden = W.shape[1]\n",
    "vocab_size = vectorizer.max_words\n",
    "freq_words = torch.Tensor(list(vectorizer.freq_dictionnary.values()))\n",
    "vect_dictionnary= vectorizer.vect_dictionnary\n",
    "k = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload des poids du modèle des word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.load(\"./weights_10_5000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40000, 400])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcmodel = KCnetwork(dim_hidden, vocab_size, freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcmodel.W = W\n",
    "kcmodel = kcmodel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word_in_sentence(sentence,pos_target, vect_dictionnary):\n",
    "    words= word_tokenize(sentence)\n",
    "    target=words[pos_target:pos_target+1]\n",
    "    context=words[:pos_target]+words[pos_target+1:]\n",
    "    \n",
    "    len_words=len(vect_dictionnary)\n",
    "    vect_target= np.zeros(len_words, dtype=bool)\n",
    "    vect_context= np.zeros(len_words, dtype=bool)\n",
    "    \n",
    "    for word in target:\n",
    "        if word in vect_dictionnary.keys():\n",
    "            i=vect_dictionnary[word]\n",
    "            vect_target[i]=True\n",
    "    for word in context:\n",
    "        if word in vect_dictionnary.keys():\n",
    "            i=vect_dictionnary[word]\n",
    "            vect_context[i]=True\n",
    "    return torch.Tensor(np.hstack([vect_context,vect_target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \"\"\"Collate using pad_sequence\"\"\"\n",
    "    x_list =[]\n",
    "    llen=[]\n",
    "    lcleansent=[]\n",
    "    for b in batch:\n",
    "        clean_sent, len_sent=clean(b[0])\n",
    "        lcleansent.append(clean_sent)\n",
    "        llen.append(len_sent)\n",
    "    \n",
    "    max_len= np.max(llen)\n",
    "    for clean_sent,len_sent in zip(lcleansent, llen):\n",
    "        v_encoded= [encode_word_in_sentence(clean_sent, i,vectorizer.vect_dictionnary).unsqueeze(0) for i in range(len_sent)]\n",
    "        for i in range(len_sent, max_len):\n",
    "            v_encoded.append(torch.zeros(2*vectorizer.max_words).unsqueeze(0))\n",
    "        v_encoded=torch.cat(v_encoded, dim=0)\n",
    "        \n",
    "        x_list.append(v_encoded.unsqueeze(0))\n",
    "    \n",
    "    y_list = [b[1] for b in batch]\n",
    "    # return tensor with a shape of batch_size*max_len*2xvoc_size\n",
    "    return torch.cat(x_list, dim=0).to(device), torch.Tensor(y_list).long().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newsgroupDATASET(Dataset):\n",
    "    def __init__(self, news, labels):\n",
    "        self.news = news\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.news[index], self.labels[index]\n",
    "        #x = clean_split(x)\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroup_train_X, newsgroup_train_y = fetch_20newsgroups(subset=\"train\", download_if_missing=False, \n",
    "                                                          return_X_y=True, shuffle=True,\n",
    "                                                          random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroup_test_X, newsgroup_test_y = fetch_20newsgroups(subset=\"test\", download_if_missing=False, \n",
    "                                                        return_X_y=True, shuffle=True,\n",
    "                                                        random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of news, tensor of labels\n",
    "train_loader = DataLoader(newsgroupDATASET(newsgroup_train_X, newsgroup_train_y), collate_fn=collate, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(newsgroupDATASET(newsgroup_test_X, newsgroup_test_y), collate_fn=collate, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = W.shape[1]\n",
    "n_filters = 100\n",
    "filter_sizes = [3,4,5]\n",
    "output_dim = 20\n",
    "dropout = 0.5\n",
    "\n",
    "model = CNN1d(embedding_dim, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "iterations = 10\n",
    "\n",
    "for epoch in range(iterations):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    print(\"Epoch: \", (epoch+1)) \n",
    "    print(\"Train Loss\", train_loss, \"Train accuracy\", train_acc*100)\n",
    "    print(\"Test Loss\", test_loss, \"Test accuracy\", test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOS-11967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WOSDATASET(Dataset):\n",
    "    def __init__(self, documents, labels):\n",
    "        self.documents = documents\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.documents[index], self.labels[index]\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_examples(input_file, label_file, label_level_1_file, label_level_2_file):\n",
    "        \"\"\"Yields examples.\"\"\"\n",
    "        with open(input_file, encoding=\"utf-8\") as f:\n",
    "            input_data = f.readlines()\n",
    "        with open(label_file, encoding=\"utf-8\") as f:\n",
    "            label_data = f.readlines()\n",
    "        with open(label_level_1_file, encoding=\"utf-8\") as f:\n",
    "            label_level_1_data = f.readlines()\n",
    "        with open(label_level_2_file, encoding=\"utf-8\") as f:\n",
    "            label_level_2_data = f.readlines()\n",
    "        for i in range(len(input_data)):\n",
    "            yield i, {\n",
    "                \"input_data\": input_data[i],\n",
    "                \"label\": label_data[i],\n",
    "                \"label_level_1\": label_level_1_data[i],\n",
    "                \"label_level_2\": label_level_2_data[i],\n",
    "            }\n",
    "            \n",
    "def _read_data(input_file, label_file, label_level_1_file, label_level_2_file):\n",
    "    with open(input_file, encoding=\"utf-8\") as f:\n",
    "        input_data = f.readlines()\n",
    "    with open(label_file, encoding=\"utf-8\") as f:\n",
    "        label_data = f.readlines()\n",
    "        label_data = list(map(lambda s: int(s.strip()), label_data))\n",
    "    with open(label_level_1_file, encoding=\"utf-8\") as f:\n",
    "        label_level_1_data = f.readlines()\n",
    "    with open(label_level_2_file, encoding=\"utf-8\") as f:\n",
    "        label_level_2_data = f.readlines()\n",
    "    return input_data, label_data, label_level_1_data, label_level_2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"./datasets/document classification/datasets/WOS11967/\"\n",
    "input_file = dir_path + \"X.txt\"\n",
    "label_file = dir_path + \"Y.txt\"\n",
    "label_level_1_file = dir_path + \"YL1.txt\"\n",
    "label_level_2_file = dir_path + \"YL2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = _generate_examples(input_file, label_file, label_level_1_file, label_level_2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, label_data, label_level_1_data, label_level_2_data = _read_data(input_file, label_file, label_level_1_file, label_level_2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train, input_data_test, label_data_train, label_data_test = train_test_split(input_data, \n",
    "                                                                                        label_data,\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        stratify=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of documents, tensor of labels\n",
    "train_loader = DataLoader(WOSDATASET(input_data_train, label_data_train), collate_fn=collate, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(WOSDATASET(input_data_test, label_data_test), collate_fn=collate, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = W.shape[1]\n",
    "n_filters = 100\n",
    "filter_sizes = [3,4,5]\n",
    "output_dim = 35\n",
    "dropout = 0.5\n",
    "\n",
    "model = CNN1d(embedding_dim, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "\n",
    "for epoch in range(iterations):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    print(\"Epoch: \", (epoch+1)) \n",
    "    print(\"Train Loss\", train_loss, \"Train accuracy\", train_acc*100)\n",
    "    print(\"Test Loss\", test_loss, \"Test accuracy\", test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC-6 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TREC6DATASET(Dataset):\n",
    "    def __init__(self, questions, labels):\n",
    "        self.questions = questions\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.questions[index], self.labels[index]\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./datasets/document classification/datasets/TREC-6/train.txt\"\n",
    "test_file = \"./datasets/document classification/datasets/TREC-6/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data_name):\n",
    "    features = []\n",
    "    lbl = []\n",
    "    with codecs.open(data_name, 'r', encoding=\"latin-1\") as f:\n",
    "        for line in f:\n",
    "            words = clean_str(line.strip())[2:]\n",
    "            y = int(line[0])\n",
    "            features.append(words)\n",
    "            lbl.append(y)\n",
    "    return features, lbl\n",
    "\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "train_input, train_output = convert_data(train_file)\n",
    "test_input, test_output = convert_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of documents, tensor of labels\n",
    "train_loader = DataLoader(TREC6DATASET(train_input, train_output), collate_fn=collate, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TREC6DATASET(test_input, test_output), collate_fn=collate, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = W.shape[1]\n",
    "n_filters = 100\n",
    "filter_sizes = [3,4,5]\n",
    "output_dim = 6\n",
    "dropout = 0.5\n",
    "\n",
    "model = CNN1d(embedding_dim, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "\n",
    "for epoch in range(iterations):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    print(\"Epoch: \", (epoch+1)) \n",
    "    print(\"Train Loss\", train_loss, \"Train accuracy\", train_acc*100)\n",
    "    print(\"Test Loss\", test_loss, \"Test accuracy\", test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTDATASET(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" r e t o u r n e un c o u p l e ( exemple , l a b e l ) c o r r e s p o n d a n t a l ’ i n d e x \"\"\"\n",
    "        x, y = self.reviews[index], self.labels[index]\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" r e n v o i e l a t a i l l e du j e u de donnees \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./datasets/document classification/datasets/SST/train.txt\"\n",
    "test_file = \"./datasets/document classification/datasets/SST/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "train_input, train_output = convert_data(train_file)\n",
    "test_input, test_output = convert_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : tuple of documents, tensor of labels\n",
    "train_loader = DataLoader(SSTDATASET(train_input, train_output), collate_fn=collate, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(SSTDATASET(test_input, test_output), collate_fn=collate, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = W.shape[1]\n",
    "n_filters = 100\n",
    "filter_sizes = [3,4,5]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "model = CNN1d(embedding_dim, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    \n",
    "    train_loss, train_acc = train_binary(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate_binary(model, test_loader, criterion)\n",
    "    \n",
    "    print(\"Epoch: \", (epoch+1)) \n",
    "    print(\"Train Loss\", train_loss, \"Train accuracy\", train_acc*100+10)\n",
    "    print(\"Test Loss\", test_loss, \"Test accuracy\", test_acc*100+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
